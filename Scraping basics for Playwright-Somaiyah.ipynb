{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping basics for Playwright\n",
    "\n",
    "This notebook is a combination of small scraping techniques along with how to use Playwright. Along with the class notes, the [scraping section](https://jonathansoma.com/everything/scraping/) on my Everything I Know site might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import what you need to use Playwright, and start up a new browser to use for scraping. \n",
    "\n",
    "> If you end up opening a lot of Chromes/Chromiums, shutting down the Python kernel with the stop button is an easy way to make them go away! You'll have to re-run your notebook, but at least you won't have sixty icons in your dock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /Users/somaiyah/.pyenv/versions/3.12.7/lib/python3.12/site-packages (1.48.0)\n",
      "Requirement already satisfied: greenlet==3.1.1 in /Users/somaiyah/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: pyee==12.0.0 in /Users/somaiyah/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from playwright) (12.0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/somaiyah/.pyenv/versions/3.12.7/lib/python3.12/site-packages (from pyee==12.0.0->playwright) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install playwright\n",
    "!playwright install\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright.async_api import async_playwright\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping by class\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-class.html using their **class name**, printing out the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/by-class.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/by-class.html' method='GET'>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/by-class.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = await page.content()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to Scrape Things'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup.find(class_ = 'title')\n",
    "title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Probably using Playwright'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subhead = soup.find(class_ = 'subhead')\n",
    "subhead.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By Jonathan Soma'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byline = soup.find(class_ = 'byline')\n",
    "byline.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping using a single tag\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-list.html, creating a dictionary out of the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/by-list.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/by-list.html' method='GET'>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/by-list.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = await page.content()\n",
    "soup2 = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Things',\n",
       " 'subhead': 'Probably using Playwright',\n",
       " 'byline': 'By Jonathan Soma'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_info = soup2.find_all('p')\n",
    "for info in all_info:\n",
    "    dictionary= {}\n",
    "    dictionary['title'] = all_info[0].text\n",
    "    dictionary['subhead'] = all_info[1].text\n",
    "    dictionary['byline'] = all_info[2].text\n",
    "dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html just like you above, but use  **wait_for** to wait for the text \"Everything has shown up\" to show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html' method='GET'>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     dictionary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m all_info[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m      8\u001b[0m     dictionary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubhead\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m all_info[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m----> 9\u001b[0m     dictionary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbyline\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mall_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     10\u001b[0m     dictionary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddendum\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m all_info[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     11\u001b[0m dictionary\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "html = await page.content()\n",
    "soup3 = BeautifulSoup(html, 'html.parser')\n",
    "await page.get_by_text(\"Everything has shown up\").wait_for()\n",
    "all_info = soup3.find_all('p')\n",
    "for info in all_info:\n",
    "    dictionary= {}\n",
    "    dictionary['title'] = all_info[0].text\n",
    "    dictionary['subhead'] = all_info[1].text\n",
    "    dictionary['byline'] = all_info[2].text\n",
    "    dictionary['addendum'] = all_info[3].text\n",
    "dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forms\n",
    "\n",
    "Display the content of the `h1` tag on http://jonathansoma.com/columbia/interactive-scrape/inputs.html. You'll need to follow the instructions to complete the form first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You did it\n"
     ]
    }
   ],
   "source": [
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/inputs.html\")\n",
    "await page.locator('#best-animal').fill('cat')\n",
    "await page.locator('select').select_option('Open')\n",
    "await page.locator('#submit').click()\n",
    "html = await page.content()\n",
    "soup4 = BeautifulSoup(html, 'html.parser')\n",
    "h1 = soup4.find('h1')\n",
    "print(h1.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a single table row\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html, creating a dictionary out of the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/single-table-row.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/single-table-row.html' method='GET'>>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/single-table-row.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Things',\n",
       " 'subhead': 'Probably using Playwright',\n",
       " 'byline': 'By Jonathan Soma'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "soup5 = BeautifulSoup(html, 'html.parser')\n",
    "all_info = soup5.find_all('td')\n",
    "for info in all_info:\n",
    "    dictionary= {}\n",
    "    dictionary['title'] = all_info[0].text\n",
    "    dictionary['subhead'] = all_info[1].text\n",
    "    dictionary['byline'] = all_info[2].text\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into a dictionary\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html, saving the title, subhead, and byline into a single dictionary called `book`.\n",
    "\n",
    "> Don't use pandas for this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Things',\n",
       " 'subhead': 'Probably using Playwright',\n",
       " 'byline': 'By Jonathan Soma'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for info in all_info:\n",
    "    book= {}\n",
    "    book['title'] = all_info[0].text\n",
    "    book['subhead'] = all_info[1].text\n",
    "    book['byline'] = all_info[2].text\n",
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple table rows\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html, creating a list of dictionaries. Convert to a pandas dataframe with `pd.json_normalize`. Save it as `output.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Things',\n",
       " 'subhead': 'Probably using Playwright',\n",
       " 'byline': 'By Jonathan Soma'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html\")\n",
    "await page.wait_for_selector('tr')\n",
    "html = await page.content()\n",
    "soup6 = BeautifulSoup(html, 'html.parser')\n",
    "all_info = soup6.find_all('tr')\n",
    "info=all_info[0]\n",
    "soma = {}\n",
    "soma['title'] = info.find_all('td')[0].text\n",
    "soma['subhead'] = info.find_all('td')[1].text\n",
    "soma['byline'] = info.find_all('td')[2].text\n",
    "soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Many Things',\n",
       " 'subhead': 'But, Is It Even Possible?',\n",
       " 'byline': 'By Sonathan Joma'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = all_info[1]\n",
    "joma = {}\n",
    "joma['title'] = info.find_all('td')[0].text\n",
    "joma['subhead'] = info.find_all('td')[1].text\n",
    "joma['byline'] = info.find_all('td')[2].text\n",
    "joma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The End of Scraping',\n",
       " 'subhead': \"Let's All Use CSV Files\",\n",
       " 'byline': 'By Amos Nathanos'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = all_info[2]\n",
    "nathanos = {}\n",
    "nathanos['title'] = info.find_all('td')[0].text\n",
    "nathanos['subhead'] = info.find_all('td')[1].text\n",
    "nathanos['byline'] = info.find_all('td')[2].text\n",
    "nathanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'How to Scrape Things',\n",
       "  'subhead': 'Probably using Playwright',\n",
       "  'byline': 'By Jonathan Soma'},\n",
       " {'title': 'How to Scrape Many Things',\n",
       "  'subhead': 'But, Is It Even Possible?',\n",
       "  'byline': 'By Sonathan Joma'},\n",
       " {'title': 'The End of Scraping',\n",
       "  'subhead': \"Let's All Use CSV Files\",\n",
       "  'byline': 'By Amos Nathanos'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_list = [soma, joma, nathanos]\n",
    "scraped_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subhead</th>\n",
       "      <th>byline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Scrape Things</td>\n",
       "      <td>Probably using Playwright</td>\n",
       "      <td>By Jonathan Soma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Scrape Many Things</td>\n",
       "      <td>But, Is It Even Possible?</td>\n",
       "      <td>By Sonathan Joma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The End of Scraping</td>\n",
       "      <td>Let's All Use CSV Files</td>\n",
       "      <td>By Amos Nathanos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title                    subhead            byline\n",
       "0       How to Scrape Things  Probably using Playwright  By Jonathan Soma\n",
       "1  How to Scrape Many Things  But, Is It Even Possible?  By Sonathan Joma\n",
       "2        The End of Scraping    Let's All Use CSV Files  By Amos Nathanos"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.json_normalize(scraped_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scraped_list).to_csv('output.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping an actual table\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/the-actual-table.html using pandas' HTML reading function. Save it as `output.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "html = await page.content()\n",
    "!pip install --quiet html5lib lxml\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "table = soup.find('table')\n",
    "html_io = StringIO(str(table))\n",
    "dfs = pd.read_html(html_io)\n",
    "df = dfs[0]\n",
    "df.to_csv(\"output2.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `html.parser` vs `html5lib`\n",
    "\n",
    "Here is some good HTML:\n",
    "\n",
    "```python\n",
    "html_good = \"\"\"\n",
    "<h1>This is a title</h1>\n",
    "<h2>This is a subhead</h2>\n",
    "<p>This is a paragraph</p>\n",
    "<p>This is another paragraph</p>\n",
    "\"\"\"\n",
    "\n",
    "Here is some bad HTML:\n",
    "    \n",
    "html_bad = \"\"\"\n",
    "<h1>This is a title\n",
    "<h2>This is a subhead\n",
    "<p>This is a paragraph\n",
    "<p>This is another paragraph\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "When you're using BeautifulSoup, you can use different parsers, including `html.parser`, `html5lib` and `lxml`. Try both the good HTML and bad HTML with each parser and use `print(soup_doc.prettify())` to view the difference.\n",
    "\n",
    "What is different about each one?\n",
    "\n",
    "> You'll need to `pip install` for both html5lib and lxml. Since you aren't important them, they're coming from BeautifulSoup, you'll need to do **Kernel > Restart** and run from the top after installing to have them work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parser: html.parser\n",
      "Good HTML:\n",
      "<h1>\n",
      " This is a title\n",
      "</h1>\n",
      "<h2>\n",
      " This is a subhead\n",
      "</h2>\n",
      "<p>\n",
      " This is a paragraph\n",
      "</p>\n",
      "<p>\n",
      " This is another paragraph\n",
      "</p>\n",
      "\n",
      "Bad HTML:\n",
      "<h1>\n",
      " This is a title\n",
      " <h2>\n",
      "  This is a subhead\n",
      "  <p>\n",
      "   This is a paragraph\n",
      "   <p>\n",
      "    This is another paragraph\n",
      "   </p>\n",
      "  </p>\n",
      " </h2>\n",
      "</h1>\n",
      "\n",
      "Using parser: html5lib\n",
      "Good HTML:\n",
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   This is a title\n",
      "  </h1>\n",
      "  <h2>\n",
      "   This is a subhead\n",
      "  </h2>\n",
      "  <p>\n",
      "   This is a paragraph\n",
      "  </p>\n",
      "  <p>\n",
      "   This is another paragraph\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "Bad HTML:\n",
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   This is a title\n",
      "  </h1>\n",
      "  <h2>\n",
      "   This is a subhead\n",
      "   <p>\n",
      "    This is a paragraph\n",
      "   </p>\n",
      "   <p>\n",
      "    This is another paragraph\n",
      "   </p>\n",
      "  </h2>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "Using parser: lxml\n",
      "Good HTML:\n",
      "<html>\n",
      " <body>\n",
      "  <h1>\n",
      "   This is a title\n",
      "  </h1>\n",
      "  <h2>\n",
      "   This is a subhead\n",
      "  </h2>\n",
      "  <p>\n",
      "   This is a paragraph\n",
      "  </p>\n",
      "  <p>\n",
      "   This is another paragraph\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "Bad HTML:\n",
      "<html>\n",
      " <body>\n",
      "  <h1>\n",
      "   This is a title\n",
      "   <h2>\n",
      "    This is a subhead\n",
      "   </h2>\n",
      "  </h1>\n",
      "  <p>\n",
      "   This is a paragraph\n",
      "  </p>\n",
      "  <p>\n",
      "   This is another paragraph\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Good HTML\n",
    "html_good = \"\"\"\n",
    "<h1>This is a title</h1>\n",
    "<h2>This is a subhead</h2>\n",
    "<p>This is a paragraph</p>\n",
    "<p>This is another paragraph</p>\n",
    "\"\"\"\n",
    "# Bad HTML\n",
    "html_bad = \"\"\"\n",
    "<h1>This is a title\n",
    "<h2>This is a subhead\n",
    "<p>This is a paragraph\n",
    "<p>This is another paragraph\n",
    "\"\"\"\n",
    "parsers = ['html.parser', 'html5lib', 'lxml']\n",
    "for parser in parsers:\n",
    "    print(\"Using parser:\", parser)\n",
    "    soup_good = BeautifulSoup(html_good, parser)\n",
    "    print(\"Good HTML:\")\n",
    "    print(soup_good.prettify())\n",
    "    soup_bad = BeautifulSoup(html_bad, parser)\n",
    "    print(\"Bad HTML:\")\n",
    "    print(soup_bad.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
